{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DATA_DIR: c:\\Users\\aryan\\Desktop\\Mindflix_AI_Aryan_Jain_B22092\\data\n",
      "‚úÖ VIDEO_DIR: c:\\Users\\aryan\\Desktop\\Mindflix_AI_Aryan_Jain_B22092\\data\\videos\n",
      "‚úÖ AUDIO_DIR: c:\\Users\\aryan\\Desktop\\Mindflix_AI_Aryan_Jain_B22092\\data\\audios\n",
      "‚úÖ KEYFRAMES_DIR: c:\\Users\\aryan\\Desktop\\Mindflix_AI_Aryan_Jain_B22092\\data\\keyframes\n",
      "‚úÖ TRANSCRIPTS_DIR: c:\\Users\\aryan\\Desktop\\Mindflix_AI_Aryan_Jain_B22092\\data\\transcripts\n",
      "‚úÖ VideoRAG initialized successfully.\n",
      "‚úÖ Skipping https://www.youtube.com/watch?v=Kf57KGwKa0w (Already Processed)\n",
      "‚úÖ Skipping https://www.youtube.com/watch?v=ftDsSB3F5kg (Already Processed)\n",
      "‚úÖ Skipping https://www.youtube.com/watch?v=kKFrbhZGNNI (Already Processed)\n",
      "‚úÖ Skipping https://www.youtube.com/watch?v=6qUxwZcTXHY (Already Processed)\n",
      "üé¨ Processing: https://www.youtube.com/watch?v=MspNdsh0QcM\n",
      "‚úÖ Video exists: c:\\Users\\aryan\\Desktop\\Mindflix_AI_Aryan_Jain_B22092\\data\\videos\\2aab073be753413b4a9c63d8b3b25403.mp4. Skipping download.\n",
      "Checking for existing transcript at: c:\\Users\\aryan\\Desktop\\Mindflix_AI_Aryan_Jain_B22092\\data\\transcripts\\2aab073be753413b4a9c63d8b3b25403.json\n",
      "c:\\Users\\aryan\\Desktop\\Mindflix_AI_Aryan_Jain_B22092\\data\\transcripts\\2aab073be753413b4a9c63d8b3b25403.json was not present.\n",
      "üöÄ Transcribing audio: c:\\Users\\aryan\\Desktop\\Mindflix_AI_Aryan_Jain_B22092\\data\\audios\\2aab073be753413b4a9c63d8b3b25403.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aryan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yt_dlp\n",
    "import chromadb\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import whisper\n",
    "\n",
    "\n",
    "class VideoRAG:\n",
    "    def __init__(self, video_links, chromadb_path=\"chromadb\"):\n",
    "        \"\"\"Initialize directories, models, and database connection.\"\"\"\n",
    "        self.video_links = video_links\n",
    "\n",
    "        # ‚úÖ Get the absolute path of the backend directory (where this script runs)\n",
    "        self.backend_dir = os.getcwd()  # Replaces __file__ for Jupyter Notebook compatibility\n",
    "\n",
    "        # ‚úÖ Move one level up to find `data/` as a sibling of `backend/`\n",
    "        self.data_dir = os.path.abspath(os.path.join(self.backend_dir, \"..\", \"data\"))\n",
    "\n",
    "        # ‚úÖ Define directory paths inside `data/`\n",
    "        self.video_dir = os.path.join(self.data_dir, \"videos\")\n",
    "        self.audio_dir = os.path.join(self.data_dir, \"audios\")\n",
    "        self.keyframes_dir = os.path.join(self.data_dir, \"keyframes\")\n",
    "        self.transcripts_dir = os.path.join(self.data_dir, \"transcripts\")\n",
    "\n",
    "        # ‚úÖ Create directories if they don't exist\n",
    "        for directory in [self.video_dir, self.audio_dir, self.keyframes_dir, self.transcripts_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # ‚úÖ Initialize models\n",
    "        self.sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        self.whisper_model = whisper.load_model(\"medium\")  # ‚úÖ Load once\n",
    "\n",
    "        # ‚úÖ Initialize ChromaDB\n",
    "        self.client = chromadb.PersistentClient(path=chromadb_path)\n",
    "        self.collection = self.client.get_or_create_collection(name=\"video_transcripts\")\n",
    "\n",
    "        # ‚úÖ Debugging: Print paths to verify correctness\n",
    "        print(f\"‚úÖ DATA_DIR: {self.data_dir}\")\n",
    "        print(f\"‚úÖ VIDEO_DIR: {self.video_dir}\")\n",
    "        print(f\"‚úÖ AUDIO_DIR: {self.audio_dir}\")\n",
    "        print(f\"‚úÖ KEYFRAMES_DIR: {self.keyframes_dir}\")\n",
    "        print(f\"‚úÖ TRANSCRIPTS_DIR: {self.transcripts_dir}\")\n",
    "        print(\"‚úÖ VideoRAG initialized successfully.\")\n",
    "\n",
    "\n",
    "    def generate_video_id(self, video_url):\n",
    "        \"\"\"Generate a unique hash-based video ID.\"\"\"\n",
    "        return hashlib.md5(video_url.encode()).hexdigest()\n",
    "\n",
    "    def check_video_exists(self, video_id):\n",
    "        \"\"\"Check if a video and its metadata already exist.\"\"\"\n",
    "        transcript_path = os.path.join(self.transcripts_dir, f\"{video_id}.json\")\n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        existing_data = self.collection.get()\n",
    "        existing_ids = set(existing_data.get(\"ids\", []))\n",
    "\n",
    "        return f\"{video_id}_0\" in existing_ids and os.path.exists(transcript_path) and os.path.exists(video_path)\n",
    "\n",
    "    def download_video(self, video_url, video_id):\n",
    "        \"\"\"Download video if not already present.\"\"\"\n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        if os.path.exists(video_path):\n",
    "            print(f\"‚úÖ Video exists: {video_path}. Skipping download.\")\n",
    "            return video_path\n",
    "\n",
    "        print(f\"‚¨áÔ∏è Downloading video: {video_url}\")\n",
    "        ydl_opts = {\"outtmpl\": video_path, \"format\": \"best\", \"noplaylist\": True}\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([video_url])\n",
    "\n",
    "        return video_path\n",
    "\n",
    "    def extract_audio(self, video_id):\n",
    "        \"\"\"Extract audio from video.\"\"\"\n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "        audio_path = os.path.join(self.audio_dir, f\"{video_id}.wav\")\n",
    "\n",
    "        if os.path.exists(audio_path):\n",
    "            return audio_path\n",
    "\n",
    "        command = [\"ffmpeg\", \"-i\", video_path, \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", audio_path]\n",
    "        subprocess.run(command, check=True)\n",
    "        return audio_path\n",
    "\n",
    "    def transcribe_audio(self, audio_path):\n",
    "        \"\"\"Transcribe audio using Whisper, but reuse existing transcription if available.\"\"\"\n",
    "        transcript_path = os.path.join(self.transcripts_dir, os.path.basename(audio_path).replace(\".wav\", \".json\"))\n",
    "\n",
    "        print(f\"Checking for existing transcript at: {transcript_path}\")  # Debug log\n",
    "\n",
    "        # Explicitly verify whether the transcript file exists\n",
    "        if os.path.exists(transcript_path):\n",
    "            print(f\"‚úÖ Transcript exists, loading: {transcript_path}\")\n",
    "            with open(transcript_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "            return data[\"text\"], data[\"timestamps\"]\n",
    "        else:\n",
    "            print(f\"{transcript_path} was not present.\")\n",
    "            print(f\"üöÄ Transcribing audio: {audio_path}\")  # Only logs if transcription is happening\n",
    "            result = self.whisper_model.transcribe(audio_path, word_timestamps=True)\n",
    "\n",
    "            transcript_data = {\n",
    "                \"text\": result[\"text\"],\n",
    "                \"timestamps\": [{\"start\": seg[\"start\"], \"end\": seg[\"end\"], \"text\": seg[\"text\"]} for seg in result[\"segments\"]],\n",
    "            }\n",
    "\n",
    "            # Ensure directory exists before saving\n",
    "            os.makedirs(self.transcripts_dir, exist_ok=True)\n",
    "\n",
    "            with open(transcript_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(transcript_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "            print(f\"‚úÖ Transcript saved to: {transcript_path}\")\n",
    "            return result[\"text\"], transcript_data[\"timestamps\"]\n",
    "\n",
    "    def extract_keyframes(self, video_id, interval=30):\n",
    "        \"\"\"Extract keyframes every `interval` seconds.\"\"\"\n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count, frames = 0, []\n",
    "\n",
    "        success, frame = cap.read()\n",
    "        while success:\n",
    "            if frame_count % int(frame_rate * interval) == 0:\n",
    "                frame_file = os.path.join(self.keyframes_dir, f\"{video_id}_frame_{frame_count}.jpg\")\n",
    "                cv2.imwrite(frame_file, frame)\n",
    "                frames.append(frame_file)\n",
    "            success, frame = cap.read()\n",
    "            frame_count += 1\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def generate_frame_description(self, frame_path):\n",
    "        \"\"\"Generate frame descriptions using BLIP.\"\"\"\n",
    "        raw_image = Image.open(frame_path).convert(\"RGB\")\n",
    "        inputs = self.processor(raw_image, return_tensors=\"pt\")\n",
    "        out = self.caption_model.generate(**inputs)\n",
    "        return self.processor.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    def chunk_transcript(self, transcript, transcript_times, chunk_size=200, overlap=20):\n",
    "        \"\"\"Splits transcript into smaller chunks for better retrieval.\"\"\"\n",
    "        sentences = re.split(r\"(?<=[.!?])\\s+\", transcript)  \n",
    "        chunks, chunk_timestamps = [], []\n",
    "\n",
    "        temp_chunk, temp_words = [], 0\n",
    "        start_time, end_time = None, None\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            temp_chunk.append(sentence)\n",
    "            temp_words += len(words)\n",
    "\n",
    "            if transcript_times:\n",
    "                for seg in transcript_times:\n",
    "                    if seg[\"text\"].strip() in sentence:\n",
    "                        if not start_time:\n",
    "                            start_time = seg[\"start\"]\n",
    "                        end_time = seg[\"end\"]\n",
    "\n",
    "            if temp_words >= chunk_size:\n",
    "                chunks.append(\" \".join(temp_chunk))\n",
    "                chunk_timestamps.append({\"start_time\": start_time, \"end_time\": end_time})\n",
    "                temp_chunk = temp_chunk[-overlap:]\n",
    "                temp_words = sum(len(sent.split()) for sent in temp_chunk)\n",
    "                start_time, end_time = None, None\n",
    "\n",
    "        if temp_chunk:\n",
    "            chunks.append(\" \".join(temp_chunk))\n",
    "            chunk_timestamps.append({\"start_time\": start_time, \"end_time\": end_time})\n",
    "\n",
    "        return chunks, chunk_timestamps\n",
    "\n",
    "    def store_metadata(self, video_id, video_url, transcript, transcript_times):\n",
    "        \"\"\"Store transcript and metadata into ChromaDB.\"\"\"\n",
    "        chunks, chunk_timestamps = self.chunk_transcript(transcript, transcript_times)\n",
    "        chunk_metadatas = [{\"video_url\": video_url, \"start_time\": chunk_timestamps[i][\"start_time\"], \"end_time\": chunk_timestamps[i][\"end_time\"]} for i in range(len(chunks))]\n",
    "\n",
    "        chunk_vectors = self.sentence_model.encode(chunks)\n",
    "        self.collection.add(documents=chunks, metadatas=chunk_metadatas, embeddings=chunk_vectors, ids=[f\"{video_id}_{i}\" for i in range(len(chunks))])\n",
    "\n",
    "    def retrieve_answer(self, query, confidence_threshold=0.5):\n",
    "        \"\"\"Retrieve answers based on semantic similarity.\"\"\"\n",
    "        query_vector = self.sentence_model.encode(query)\n",
    "        results = self.collection.query(query_embeddings=[query_vector], n_results=10)\n",
    "\n",
    "        response_candidates = []\n",
    "        for doc_list, metadata_list, score_list in zip(results[\"documents\"], results[\"metadatas\"], results[\"distances\"]):\n",
    "            for doc, metadata, score in zip(doc_list, metadata_list or [{}], score_list):\n",
    "                if score >= confidence_threshold:\n",
    "                    response_candidates.append((score, f\"üé¨ Video URL: {metadata.get('video_url', 'Unknown')} ‚è≥ Start Time: {metadata.get('start_time', 'Unknown')}s\\nüìú Excerpt: {doc}\"))\n",
    "\n",
    "        response_candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "        return response_candidates[0][1] if response_candidates else \"‚ùå No high-confidence results found.\"\n",
    "\n",
    "    def process_videos(self):\n",
    "        \"\"\"Loops through video list and processes them only if metadata is missing in ChromaDB.\"\"\"\n",
    "        for video_url in self.video_links:\n",
    "            video_id = self.generate_video_id(video_url)\n",
    "            if self.check_video_exists(video_id):\n",
    "                print(f\"‚úÖ Skipping {video_url} (Already Processed)\")\n",
    "                continue\n",
    "            print(f\"üé¨ Processing: {video_url}\")\n",
    "            video_path = self.download_video(video_url, video_id)\n",
    "            audio_path = self.extract_audio(video_id)\n",
    "            transcript, transcript_times = self.transcribe_audio(audio_path)\n",
    "            keyframes = self.extract_keyframes(video_id)\n",
    "            keyframe_descriptions = [self.generate_frame_description(frame) for frame in keyframes]\n",
    "            self.store_metadata(video_id, video_url, transcript, transcript_times)\n",
    "            print(f\"‚úÖ Processed and stored metadata for: {video_url}\")\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "video_links = [\n",
    "    \"https://www.youtube.com/watch?v=Kf57KGwKa0w\",\n",
    "    \"https://www.youtube.com/watch?v=ftDsSB3F5kg\",\n",
    "    \"https://www.youtube.com/watch?v=kKFrbhZGNNI\",\n",
    "    \"https://www.youtube.com/watch?v=6qUxwZcTXHY\",\n",
    "    \"https://www.youtube.com/watch?v=MspNdsh0QcM\"\n",
    "]\n",
    "\n",
    "video_rag = VideoRAG(video_links)\n",
    "video_rag.process_videos()\n",
    "\n",
    "@app.route(\"/query\", methods=[\"POST\"])\n",
    "def query():\n",
    "    return jsonify({\"responses\": [video_rag.retrieve_answer(q) for q in request.get_json().get(\"queries\", [])]})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=8084, debug=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
